import os
import time
import pickle as pkl
import scipy
import numpy as np

from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split


fid_dict = pkl.load(open('./dataset/fid_dict.pkl', 'rb'))

with open('./dataset/training-set.csv') as f:
    train_row = [l.split(',') for l in f.read().strip().split('\n')]
    train_row = [[fid_dict[l[0]], int(l[1])] for l in train_row if l[0] in fid_dict]
    train_dict = dict(train_row)
    train_row = np.array(train_row)
    positive_fid = set(train_row[train_row[:, 1] == 1, 0])
    negative_fid = set(train_row[train_row[:, 1] == 0, 0])

with open('./dataset/testing-set.csv') as f:
    test_row = [l.split(',') for l in f.read().strip().split('\n')]
    test_row = [l for l in test_row if l[0] in fid_dict]


feature_sz = pkl.load(open('./dataset/features/feature_sz.pkl', 'rb'))
feature_samec_diffp = pkl.load(open('./dataset/features/feature_samec_diffp.pkl', 'rb'))
feature_num_distinct_target = pkl.load(open('./dataset/features/feature_num_distinct_target.pkl', 'rb'))
feature_time_elps = pkl.load(open('./dataset/features/feature_time_elps.pkl', 'rb'))
feature_interval = pkl.load(open('./dataset/features/feature_interval.pkl', 'rb'))
feature_interval_std = pkl.load(open('./dataset/features/feature_interval_std.pkl', 'rb'))
feature_avg_sz_4cid = pkl.load(open('./dataset/features/feature_avg_sz_4cid.pkl', 'rb'))
feature_time_elps_4cid = pkl.load(open('./dataset/features/feature_time_elps_4cid.pkl', 'rb'))
feature_avg_interval_4cid = pkl.load(open('./dataset/features/feature_avg_interval_4cid.pkl', 'rb'))
feature_std_interval_4cid = pkl.load(open('./dataset/features/feature_std_interval_4cid.pkl', 'rb'))
feature_bag_of_pid = pkl.load(open('./dataset/features/feature_bag_of_pid.pkl', 'rb'))
feature_bag_of_pid_4cid = pkl.load(open('./dataset/features/feature_bag_of_pid_4cid.pkl', 'rb'))
feature_oneshot_num = pkl.load(open('./dataset/features/feature_oneshot_num.pkl', 'rb'))
feature_oneshot_p = pkl.load(open('./dataset/features/feature_oneshot_p.pkl', 'rb'))
feature_unigram = pkl.load(open('./dataset/features/feature_unigram.pkl', 'rb'))
feature_bigram = pkl.load(open('./dataset/features/feature_bigram.pkl', 'rb'))
feature_unigram_entropy = pkl.load(open('./dataset/features/feature_unigram_entropy.pkl', 'rb'))
feature_bigram_entropy = pkl.load(open('./dataset/features/feature_bigram_entropy.pkl', 'rb'))
feature_max_retry_allday = pkl.load(open('./dataset/features/feature_max_retry_allday.pkl', 'rb'))
feature_max_retry_perday = pkl.load(open('./dataset/features/feature_max_retry_perday.pkl', 'rb'))
feature_max_retry_perhour = pkl.load(open('./dataset/features/feature_max_retry_perhour.pkl', 'rb'))
feature_max_retry_perminutes = pkl.load(open('./dataset/features/feature_max_retry_perminutes.pkl', 'rb'))
cid_csr_matrix_col_sel_greedy_5fold = pkl.load(open('./dataset/features/cid_csr_matrix_col_sel_greedy_5fold.pkl', 'rb'))

bag_of_pid_sel_col = feature_bag_of_pid[train_row[:, 0]].sum(axis=0) > 1000
X_simple = np.hstack([
    feature_sz.reshape(-1, 1),
    feature_samec_diffp.reshape(-1, 1),
    feature_num_distinct_target.reshape(-1, 1),
    feature_time_elps.reshape(-1, 1),
    feature_interval.reshape(-1, 1),
    feature_interval_std.reshape(-1, 1),
    feature_avg_sz_4cid.reshape(-1, 1),
    feature_time_elps_4cid.reshape(-1, 1),
    feature_avg_interval_4cid.reshape(-1, 1),
    feature_std_interval_4cid.reshape(-1, 1),
    feature_oneshot_p.reshape(-1, 1),
    feature_bag_of_pid[:, bag_of_pid_sel_col],
    feature_bag_of_pid_4cid[:, bag_of_pid_sel_col],
    feature_unigram,
    feature_bigram,
    feature_unigram_entropy.reshape(-1, 1),
    feature_bigram_entropy.reshape(-1, 1),
    feature_max_retry_allday.reshape(-1, 1),
    feature_max_retry_perday.reshape(-1, 1),
    feature_max_retry_perhour.reshape(-1, 1),
    feature_max_retry_perminutes.reshape(-1, 1),
])
X_cid = (cid_csr_matrix_col_sel_greedy_5fold > 0).astype(np.uint8)

assert(X_simple.shape[0] == 81894)
assert(X_cid.shape[0] == 81894)
print('X_simple.shape:', X_simple.shape)
print('X_cid.shape:', X_cid.shape)


import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.utils.data as data
from torch.autograd import Variable

use_cuda = True


if use_cuda and not torch.cuda.is_available():
    raise Exception('cuda is not available')
FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor
LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor
ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor
Tensor = FloatTensor



class X_Dataset(data.Dataset):
    def __init__(self, x_simple, x_cid, ys):
        self.x_simple = x_simple
        self.x_cid = x_cid
        self.ys = ys.astype(np.float)
        
        # Mask useless x_simple
        self.x_mu = np.mean(x_simple, axis=0)
        self.x_gamma = np.std(x_simple, axis=0)
        self.simple_mask = (self.x_gamma > 1e-2).astype(np.float32)
        
        # Mask useless x_cid
        self.cid_mask = np.array(x_cid.sum(axis=0)!=0, dtype=np.uint8).squeeze()

    def __len__(self):
        return self.x_cid.shape[0]

    def __getitem__(self, idx):
        x_simple = self.x_simple_transformer(self.x_simple[idx])
        x_cid = self.x_cid_transformer(self.x_cid[idx].toarray())
        ys = [self.ys[idx]]
        return FloatTensor(x_simple), LongTensor(x_cid), FloatTensor(ys)
    
    def x_simple_transformer(self, x_simple):
        return (x_simple - self.x_mu) / (self.x_gamma + 1e-6) * self.simple_mask
        
    def x_cid_transformer(self, x_cid):
        return (x_cid * self.cid_mask).squeeze()
    
    
    
class Net(nn.Module):
    def __init__(self, dic_sz, emb_sz, i_sz, hid_sz, drop_rate):
        super(Net, self).__init__()
        self.emb_sz = emb_sz
        self.emb = nn.Embedding(dic_sz, emb_sz)
        layers = [
            nn.Linear(i_sz+emb_sz, hid_sz[0]),
            nn.BatchNorm1d(hid_sz[0]),
            nn.ReLU(True),
            nn.Dropout(drop_rate),
        ]
        for i in range(1, len(hid_sz)):
            layers.append(nn.Linear(hid_sz[i-1], hid_sz[i]))
            layers.append(nn.BatchNorm1d(hid_sz[i]))
            layers.append(nn.ReLU(True)),
            layers.append(nn.Dropout(drop_rate))
        layers.append(nn.Linear(hid_sz[-1], 1))
        self.layers = nn.Sequential(*layers)

    def forward(self, x_simple, x_cid):
        emb_x = self.embed_01_vec(x_cid)
        x = torch.cat([x_simple, emb_x], dim=1)
        return self.layers(x)
    
    def embed_01_vec(self, x_cid):
        x_cid = x_cid.cpu()
        x_cid = x_cid.data.numpy() if isinstance(x_cid, Variable) else x_cid.numpy()
        return torch.stack([
            self.emb(
                Variable(LongTensor(np.argwhere(r)), requires_grad=False)
            ).mean(dim=0).squeeze()
            if r.sum() != 0 else
            Variable(FloatTensor(np.zeros(self.emb_sz)), requires_grad=False)
            for r in x_cid
        ])


def train_epoch(estimator, loss_fn, optimizer, dataloader):
    estimator.train()

    for i_batch, (batch_x_simple, batch_x_cid, batch_y) in enumerate(dataloader):
        optimizer.zero_grad()
        batch_x_simple = Variable(batch_x_simple, requires_grad=False)
        batch_x_cid = Variable(batch_x_cid, requires_grad=False)
        batch_y = Variable(batch_y, requires_grad=False)
        batch_y_ = estimator(batch_x_simple, batch_x_cid)
        loss = loss_fn(batch_y_, target=batch_y)
        loss.backward()
        optimizer.step()

def gen_ans(estimator, valid_x_simple, valid_x_cid):
    estimator.eval()
    
    va_dataset = X_Dataset(valid_x_simple, valid_x_cid, np.zeros(valid_x_cid.shape[0]))
    va_loader = data.DataLoader(va_dataset, batch_size=128)

    valid_y_ = []
    for i_batch, (batch_x_simple, batch_x_cid, batch_y) in enumerate(va_loader):
        batch_x_simple = Variable(batch_x_simple, volatile=True)
        batch_x_cid = Variable(batch_x_cid, volatile=True)
        
        batch_y_ = F.sigmoid(estimator(batch_x_simple, batch_x_cid)).squeeze()
        valid_y_.extend(batch_y_.cpu().data.numpy())
    return np.array(valid_y_, dtype=np.float64)

def valid_epoch(estimator, valid_x_simple, valid_x_cid, valid_y):
    valid_y_ = gen_ans(estimator, valid_x_simple, valid_x_cid)
    return roc_auc_score(y_true=valid_y, y_score=valid_y_)

def train_model(
        x_simple_tr, x_cid_tr, y_tr,
        x_simple_va, x_cid_va, y_va,
        n_epoch, estimator, batch_size,
        optimizer, optimizer_kwargs
        ):
    x_dataset = X_Dataset(x_simple_tr, x_cid_tr, y_tr)
    dataloader = data.DataLoader(x_dataset, batch_size=batch_size, shuffle=True)
    loss_fn = nn.BCEWithLogitsLoss()
    optimizer = optimizer(estimator.parameters(), **optimizer_kwargs)

    # Start training
    best_valid_rocauc = -1
    for ith_epoch in range(n_epoch):
        train_epoch(estimator, loss_fn, optimizer, dataloader)
        valid_rocauc = valid_epoch(estimator, x_simple_va, x_cid_va, y_va)
        if valid_rocauc > best_valid_rocauc:
            best_valid_rocauc = valid_rocauc
    return best_valid_rocauc
    

for ith_ensemble in range(5000):
    # Train/Valid split
    x_simple_tr, x_simple_va, y_tr, y_va = train_test_split(
        X_simple[train_row[:, 0]], train_row[:, 1],
        test_size=1/5, random_state=ith_ensemble
    )
    x_cid_tr, x_cid_va, y_cid_tr, y_cid_va = train_test_split(
        X_cid[train_row[:, 0]], train_row[:, 1],
        test_size=1/5, random_state=ith_ensemble
    )
    assert(np.sum(y_tr == y_cid_tr) == y_tr.shape[0])
    assert(np.sum(y_va == y_cid_va) == y_va.shape[0])
    
    # Estimator
    estimator = Net(
        dic_sz=x_cid_tr.shape[-1],
        emb_sz=10,
        i_sz=x_simple_tr.shape[-1],
        hid_sz=(250, 250),
        drop_rate=0.7,
    )
    if use_cuda:
        estimator.cuda()

    best_valid_rocauc = train_model(
        x_simple_tr, x_cid_tr, y_tr,
        x_simple_va, x_cid_va, y_va,
        n_epoch=25,
        estimator=estimator,
        batch_size=16,
        optimizer=optim.Adam,
        optimizer_kwargs={}
    )
    ans = gen_ans(estimator, X_simple[len(train_row):], X_cid[len(train_row):])
    pkl.dump(ans, open('./attack/ensemble_nn_embedding/%d.pkl' % (ith_ensemble), 'wb'))
    print('%d ensemble valid score %.6f' % (ith_ensemble, best_valid_rocauc), flush=True)
    