{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pickle as pkl\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import features_loader\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Helper function for visulization\n",
    "def highlight_max_in_column(s):\n",
    "    style = ['', 'font-weight: 900; color: cyan']\n",
    "    return [style[v] for v in s == s.max()]\n",
    "\n",
    "fid_dict = pkl.load(open('./dataset/fid_dict.pkl', 'rb'))\n",
    "\n",
    "with open('./dataset/training-set.csv') as f:\n",
    "    train_row = [l.split(',') for l in f.read().strip().split('\\n')]\n",
    "    train_row = [[fid_dict[l[0]], int(l[1])] for l in train_row if l[0] in fid_dict]\n",
    "    train_dict = dict(train_row)\n",
    "    train_row = np.array(train_row)\n",
    "    positive_fid = set(train_row[train_row[:, 1] == 1, 0])\n",
    "    negative_fid = set(train_row[train_row[:, 1] == 0, 0])\n",
    "\n",
    "with open('./dataset/testing-set.csv') as f:\n",
    "    test_row = [l.split(',') for l in f.read().strip().split('\\n')]\n",
    "    test_row = [l for l in test_row if l[0] in fid_dict]\n",
    "\n",
    "with open('./dataset/features/feature_label_propagate_score.txt') as f:\n",
    "    lp_score = np.array([l.split() for l in f.read().strip().split('\\n')], dtype=np.float64)\n",
    "    lp_score = lp_score[[fid_dict[FileID] for FileID, _ in test_row]]\n",
    "    \n",
    "    \n",
    "X_norm = features_loader.load_X_norm()\n",
    "X_norm_train = X_norm[:len(train_row)]\n",
    "y_train = train_row[train_row[:, 0], 1]\n",
    "linear_reg = LinearRegression(normalize=True, n_jobs=4)\n",
    "linear_reg.fit(X_norm_train, y_train)\n",
    "y_linear_reg_test_ = linear_reg.predict(X_norm[len(train_row):])\n",
    "\n",
    "    \n",
    "\n",
    "def get_sort_score(scores):\n",
    "    to_sort = [(*s, i) for i, s in enumerate(zip(*scores))]\n",
    "    to_sort.sort()\n",
    "    \n",
    "    same_entry = len(scores[0]) - len(set([(*s, ) for s in zip(*scores)]))\n",
    "    print('Same entry before :', same_entry)\n",
    "    \n",
    "    idx = [v[-1] for v in to_sort]\n",
    "    sort_score = np.zeros(len(to_sort), dtype=np.float64)\n",
    "    sort_score[idx] = np.arange(len(to_sort)) / (len(to_sort)-1)\n",
    "    return sort_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [pkl.load(open(path, 'rb')) for path in sorted(glob.glob('./models/stacker/*'))]\n",
    "X = np.array(X).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LinearRegression(fit_intercept=True),\n",
    "    XGBClassifier(n_estimators=100, max_depth=1, random_state=277, n_jobs=8),\n",
    "    RandomForestClassifier(n_estimators=100, random_state=277, n_jobs=8),\n",
    "    ExtraTreesClassifier(n_estimators=100, random_state=277, n_jobs=8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_va, y_tr, y_va = train_test_split(X[:len(train_row)], train_row[:,1], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9704987879295377"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBClassifier(n_estimators=100, max_depth=2, subsample=0.7, random_state=277, n_jobs=8)\n",
    "model.fit(x_tr, y_tr)\n",
    "roc_auc_score(y_true=y_va, y_score=model.predict_proba(x_va)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9707276556638722"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=1000, max_depth=8, criterion='entropy', random_state=33277, n_jobs=8)\n",
    "model.fit(x_tr, y_tr)\n",
    "roc_auc_score(y_true=y_va, y_score=model.predict_proba(x_va)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709148455577379"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPClassifier((64, 32, 16), batch_size=64, activation='tanh', tol=1e-8, random_state=277)\n",
    "model.fit(x_tr, y_tr)\n",
    "roc_auc_score(y_true=y_va, y_score=model.predict_proba(x_va)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_path ./attack/ensemble_stacking/xgb_0.pkl\n",
      "fitting XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=2, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.6726205442767683)\n",
      "target_path ./attack/ensemble_stacking/rf_0.pkl\n",
      "fitting RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=8,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/mlp_0.pkl\n",
      "fitting MLPClassifier(activation='tanh', alpha=0.0001, batch_size=78, beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(64, 32, 16), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=277,\n",
      "       shuffle=True, solver='adam', tol=1e-08, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/xgb_1.pkl\n",
      "fitting XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='binary:logistic', random_state=1,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.6433056038541116)\n",
      "target_path ./attack/ensemble_stacking/rf_1.pkl\n",
      "fitting RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=8, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=8,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/mlp_1.pkl\n",
      "fitting MLPClassifier(activation='tanh', alpha=0.0001, batch_size=90, beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(64, 32, 16), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=277,\n",
      "       shuffle=True, solver='adam', tol=1e-08, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/xgb_2.pkl\n",
      "fitting XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='binary:logistic', random_state=2,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.6810680434716073)\n",
      "target_path ./attack/ensemble_stacking/rf_2.pkl\n",
      "fitting RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=9, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=8,\n",
      "            oob_score=False, random_state=2, verbose=0, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/mlp_2.pkl\n",
      "fitting MLPClassifier(activation='tanh', alpha=0.0001, batch_size=79, beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(64, 32, 16), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=277,\n",
      "       shuffle=True, solver='adam', tol=1e-08, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/xgb_3.pkl\n",
      "fitting XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='binary:logistic', random_state=3,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.6770241439244551)\n",
      "target_path ./attack/ensemble_stacking/rf_3.pkl\n",
      "fitting RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=8, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=8,\n",
      "            oob_score=False, random_state=3, verbose=0, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/mlp_3.pkl\n",
      "fitting MLPClassifier(activation='tanh', alpha=0.0001, batch_size=67, beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(64, 32, 16), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=277,\n",
      "       shuffle=True, solver='adam', tol=1e-08, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/xgb_4.pkl\n",
      "fitting XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=2, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='binary:logistic', random_state=4,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.6764922482338936)\n",
      "target_path ./attack/ensemble_stacking/rf_4.pkl\n",
      "fitting RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=8, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=8,\n",
      "            oob_score=False, random_state=4, verbose=0, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/mlp_4.pkl\n",
      "fitting MLPClassifier(activation='tanh', alpha=0.0001, batch_size=94, beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(64, 32, 16), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=277,\n",
      "       shuffle=True, solver='adam', tol=1e-08, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/xgb_5.pkl\n",
      "fitting XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='binary:logistic', random_state=5,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.6418354960329159)\n",
      "target_path ./attack/ensemble_stacking/rf_5.pkl\n",
      "fitting RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=8,\n",
      "            oob_score=False, random_state=5, verbose=0, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/mlp_5.pkl\n",
      "fitting MLPClassifier(activation='tanh', alpha=0.0001, batch_size=64, beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(64, 32, 16), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=277,\n",
      "       shuffle=True, solver='adam', tol=1e-08, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_path ./attack/ensemble_stacking/xgb_6.pkl\n",
      "fitting XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='binary:logistic', random_state=6,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.6799057415428376)\n",
      "target_path ./attack/ensemble_stacking/rf_6.pkl\n",
      "fitting RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=8, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=8,\n",
      "            oob_score=False, random_state=6, verbose=0, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/mlp_6.pkl\n",
      "fitting MLPClassifier(activation='tanh', alpha=0.0001, batch_size=81, beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(64, 32, 16), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=277,\n",
      "       shuffle=True, solver='adam', tol=1e-08, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/xgb_7.pkl\n",
      "fitting XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=2, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='binary:logistic', random_state=7,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.6477069667196059)\n",
      "target_path ./attack/ensemble_stacking/rf_7.pkl\n",
      "fitting RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=9, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=8,\n",
      "            oob_score=False, random_state=7, verbose=0, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/mlp_7.pkl\n",
      "fitting MLPClassifier(activation='tanh', alpha=0.0001, batch_size=84, beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(64, 32, 16), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=277,\n",
      "       shuffle=True, solver='adam', tol=1e-08, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/xgb_8.pkl\n",
      "fitting XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='binary:logistic', random_state=8,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.6058596601854579)\n",
      "target_path ./attack/ensemble_stacking/rf_8.pkl\n",
      "fitting RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=9, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=8,\n",
      "            oob_score=False, random_state=8, verbose=0, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/mlp_8.pkl\n",
      "fitting MLPClassifier(activation='tanh', alpha=0.0001, batch_size=58, beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(64, 32, 16), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=277,\n",
      "       shuffle=True, solver='adam', tol=1e-08, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/xgb_9.pkl\n",
      "fitting XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=2, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='binary:logistic', random_state=9,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.6683866132492216)\n",
      "target_path ./attack/ensemble_stacking/rf_9.pkl\n",
      "fitting RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=9, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=8,\n",
      "            oob_score=False, random_state=9, verbose=0, warm_start=False)\n",
      "target_path ./attack/ensemble_stacking/mlp_9.pkl\n",
      "fitting MLPClassifier(activation='tanh', alpha=0.0001, batch_size=60, beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(64, 32, 16), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=277,\n",
      "       shuffle=True, solver='adam', tol=1e-08, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    target_path = './attack/ensemble_stacking/xgb_%d.pkl' % (i)\n",
    "    if not os.path.isfile(target_path):\n",
    "        print('target_path', target_path)\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=np.random.randint(2,4),\n",
    "            subsample=np.random.uniform(0.6, 0.7),\n",
    "            random_state=i, n_jobs=8)\n",
    "        print('fitting', model)\n",
    "        model.fit(X[:len(train_row)], train_row[:,1])\n",
    "        answer = model.predict_proba(X[len(train_row):])[:,1]\n",
    "        pkl.dump(answer, open(target_path, 'wb'))\n",
    "\n",
    "    target_path = './attack/ensemble_stacking/rf_%d.pkl' % (i)\n",
    "    if not os.path.isfile(target_path):\n",
    "        print('target_path', target_path)\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=1000,\n",
    "            max_depth=np.random.randint(7,10),\n",
    "            criterion='entropy',\n",
    "            random_state=i, n_jobs=8)\n",
    "        print('fitting', model)\n",
    "        model.fit(X[:len(train_row)], train_row[:,1])\n",
    "        answer = model.predict_proba(X[len(train_row):])[:,1]\n",
    "        pkl.dump(answer, open(target_path, 'wb'))\n",
    "\n",
    "    target_path = './attack/ensemble_stacking/mlp_%d.pkl' % (i)\n",
    "    if not os.path.isfile(target_path):\n",
    "        print('target_path', target_path)\n",
    "        model = MLPClassifier(\n",
    "            (64, 32, 16),\n",
    "            batch_size=np.random.randint(50, 100),\n",
    "            activation='tanh',\n",
    "            tol=1e-8,\n",
    "            random_state=277)\n",
    "        print('fitting', model)\n",
    "        model.fit(X[:len(train_row)], train_row[:,1])\n",
    "        answer = model.predict_proba(X[len(train_row):])[:,1]\n",
    "        pkl.dump(answer, open(target_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for path in glob.glob('./attack/ensemble_stacking/*.pkl'):\n",
    "    answers.append(pkl.load(open(path, 'rb')).astype(np.float64))\n",
    "answers = np.array(answers).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_ = answers.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same entry before : 1\n"
     ]
    }
   ],
   "source": [
    "sort_score = get_sort_score([y_test_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./attack/81.csv', 'w') as f:\n",
    "    for i in range(len(test_row)):\n",
    "        f.write('%s,%.12f\\n' % (test_row[i][0], sort_score[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    29376.000000\n",
       "mean         0.073919\n",
       "std          0.171738\n",
       "min          0.000033\n",
       "25%          0.002251\n",
       "50%          0.009079\n",
       "75%          0.047233\n",
       "max          0.999807\n",
       "dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_w_lp_ = np.zeros(len(test_row))\n",
    "for i in range(len(test_row)):\n",
    "    lp, base_n, _, g_n, depth, in_edge = lp_score[i]\n",
    "    y_ = y_test_[i]\n",
    "    if base_n > 100:\n",
    "        y_test_w_lp_[i] = (lp + y_) / 2\n",
    "    elif base_n > 1:\n",
    "        y_test_w_lp_[i] = (lp * base_n + y_) / (base_n + 1)\n",
    "    else:\n",
    "        y_test_w_lp_[i] = (0 + y_) / 2\n",
    "pd.Series(y_test_w_lp_).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_svd256 = features_loader.load_X_svd256()\n",
    "X_svd256_train = X_svd256[:len(train_row)]\n",
    "y_train = train_row[:len(train_row), 1]\n",
    "print('X_svd256.shape:', X_svd256.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg = LinearRegression(normalize=True, n_jobs=4)\n",
    "linear_reg.fit(X_svd256_train, y_train)\n",
    "y_linear_ = linear_reg.predict(X_svd256[len(train_row):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlendingClassifier():\n",
    "    def __init__(self, stage_1, blender):\n",
    "        self.stage_1 = stage_1\n",
    "        self.blender = blender\n",
    "    \n",
    "    def fit(self, x, y, split_size=1/3):\n",
    "        x1, x2, y1, y2 = train_test_split(x, y, test_size=split_size, random_state=277)\n",
    "        for clf in self.stage_1:\n",
    "            clf.fit(x1, y1)\n",
    "        self.blender.fit(self.transform(x2), y2)\n",
    "        for clf in self.stage_1:\n",
    "            clf.fit(x, y)\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        if hasattr(self.blender, 'predict_proba'):\n",
    "            return self.blender.predict_proba(self.transform(x))[:,1]\n",
    "        else:\n",
    "            return self.blender.predict(self.transform(x))\n",
    "            \n",
    "    def transform(self, x):\n",
    "        return np.array([\n",
    "            clf.predict_proba(x)[:,1] if hasattr(clf, 'predict_proba') else clf.predict(x)\n",
    "            for clf in self.stage_1\n",
    "        ]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_clf = BlendingClassifier(\n",
    "    stage_1=[\n",
    "        XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            sumsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            max_delta_step=1,\n",
    "            reg_lambda=2,\n",
    "            gamma=1,\n",
    "            random_state=277, n_jobs=4, silent=True),\n",
    "        ExtraTreesClassifier(\n",
    "            n_estimators=100,\n",
    "            max_features='log2',\n",
    "            random_state=277, n_jobs=4),\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_features=45,\n",
    "            class_weight='balanced_subsample',\n",
    "            criterion='entropy',\n",
    "            oob_score=True,\n",
    "            random_state=277, n_jobs=4),\n",
    "        AdaBoostClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=277)\n",
    "    ],\n",
    "    blender=LinearRegression(fit_intercept=False, n_jobs=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment setting & Model setting\n",
    "n_fold = 5\n",
    "\n",
    "# Validation splitter\n",
    "skf = StratifiedKFold(n_splits=n_fold, random_state=277)\n",
    "\n",
    "# Running experiment\n",
    "mean_all_record = []\n",
    "for i_fold, (tr_idx, va_idx) in enumerate(skf.split(np.arange(len(y_train)), y_train)):\n",
    "    fold_x_tr = X_svd256_train[tr_idx]\n",
    "    fold_x_va = X_svd256_train[va_idx]\n",
    "    fold_y_tr = y_train[tr_idx]\n",
    "    fold_y_va = y_train[va_idx]\n",
    "    \n",
    "    blend_clf.fit(fold_x_tr, fold_y_tr)\n",
    "    blend_y_ = blend_clf.predict_proba(fold_x_va)\n",
    "    print(roc_auc_score(y_true=fold_y_va, y_score=blend_y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_clf = BlendingClassifier(\n",
    "    stage_1=[\n",
    "        XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            sumsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            max_delta_step=1,\n",
    "            reg_lambda=2,\n",
    "            gamma=1,\n",
    "            random_state=277, n_jobs=4, silent=True),\n",
    "        ExtraTreesClassifier(\n",
    "            n_estimators=100,\n",
    "            max_features='log2',\n",
    "            random_state=277, n_jobs=4),\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_features=45,\n",
    "            class_weight='balanced_subsample',\n",
    "            criterion='entropy',\n",
    "            oob_score=True,\n",
    "            random_state=277, n_jobs=4),\n",
    "        AdaBoostClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=277)\n",
    "    ],\n",
    "    blender=LinearRegression(fit_intercept=False, n_jobs=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_clf.fit(X_svd256_train, y_train)\n",
    "blend_y_ = blend_clf.predict_proba(X_svd256[len(train_row):])\n",
    "sort_score = get_sort_score([blend_y_, y_linear_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any Blending (Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_clf = BlendingClassifier(\n",
    "    stage_1=[\n",
    "        XGBClassifier(\n",
    "            n_estimators=1000,\n",
    "            max_depth=10,\n",
    "            sumsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            max_delta_step=1,\n",
    "            reg_lambda=2,\n",
    "            gamma=1,\n",
    "            random_state=277, n_jobs=4, silent=True),\n",
    "        ExtraTreesClassifier(\n",
    "            n_estimators=1000,\n",
    "            max_features='log2',\n",
    "            random_state=277, n_jobs=4),\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=1000,\n",
    "            max_features=45,\n",
    "            class_weight='balanced_subsample',\n",
    "            criterion='entropy',\n",
    "            oob_score=True,\n",
    "            random_state=277, n_jobs=4),\n",
    "        AdaBoostClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=277)\n",
    "    ],\n",
    "    blender=RandomForestClassifier(n_estimators=1000, random_state=277, n_jobs=4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_clf.fit(X_svd256_train, y_train)\n",
    "blend_y_ = blend_clf.predict_proba(X_svd256[len(train_row):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./attack/74.csv', 'w') as f:\n",
    "    for i in range(len(test_row)):\n",
    "        f.write('%s,%.12f\\n' % (test_row[i][0], sort_score[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
