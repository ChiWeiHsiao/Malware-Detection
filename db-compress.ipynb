{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Compress by Remapping\n",
    "- remap `FileID`, `CustomerID`, `ProductID` to interger\n",
    "- Origin\n",
    "    - `dataset/query_log/*`: 6.6G\n",
    "- Resulting\n",
    "    - `dataset/query_log.npy`: 1.6G\n",
    "    - `dataset/fid_dict.pkl`: 3.6M\n",
    "    - `dataset/cid_dict.pkl`: 249M\n",
    "    - `dataset/fid_dict.pkl`: 4K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dict for re-mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input path\n",
    "train_path = './dataset/training-set.csv'\n",
    "test_path = './dataset/testing-set.csv'\n",
    "exception_train_path = './dataset/exception_train.txt'\n",
    "exception_test_path = './dataset/exception_testing.txt'\n",
    "query_log_path = './dataset/query_log/'\n",
    "\n",
    "# Output path\n",
    "fid_dict_path = './dataset/fid_dict.pkl'\n",
    "cid_dict_path = './dataset/cid_dict.pkl'\n",
    "pid_dict_path = './dataset/pid_dict.pkl'\n",
    "\n",
    "npy_path = './dataset/query_log.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "excp = set()\n",
    "\n",
    "with open(exception_train_path) as f:\n",
    "    excp = excp.union(set(f.read().split()))\n",
    "    \n",
    "with open(exception_test_path) as f:\n",
    "    excp = excp.union(set(f.read().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existed fid_dict\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(fid_dict_path):\n",
    "    print('Loading existed fid_dict')\n",
    "    fid_dict = pkl.load(open(fid_dict_path, 'rb'))\n",
    "else:\n",
    "    print('Creating new fid_dict')\n",
    "    fid_dict = {}\n",
    "    with open(train_path) as f:\n",
    "        for fid in [l.split(',')[0] for l in f.read().strip().split('\\n')]:\n",
    "            if fid in excp:\n",
    "                continue\n",
    "            assert(fid not in fid_dict)\n",
    "            fid_dict[fid] = len(fid_dict)\n",
    "\n",
    "    with open(test_path) as f:\n",
    "        for fid in [l.split(',')[0] for l in f.read().strip().split('\\n')]:\n",
    "            if fid in excp:\n",
    "                continue\n",
    "            assert(fid not in fid_dict)\n",
    "            fid_dict[fid] = len(fid_dict)\n",
    "    pkl.dump(fid_dict, open(fid_dict_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new cid_dict, pid_dict\n",
      "Processing 0531.csv\r"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(cid_dict_path):\n",
    "    print('Loading existed cid_dict, pid_dict')\n",
    "    cid_dict = pkl.load(open(cid_dict_path, 'rb'))\n",
    "    pid_dict = pkl.load(open(pid_dict_path, 'rb'))\n",
    "else:\n",
    "    print('Creating new cid_dict, pid_dict')\n",
    "    cid_dict = {}\n",
    "    pid_dict = {}\n",
    "    for fname in sorted(os.listdir(query_log_path)):\n",
    "        print('Processing', fname, end='\\r', flush=True)\n",
    "        if not fname.endswith('.csv'):\n",
    "            continue\n",
    "        with open(os.path.join(query_log_path, fname)) as f:\n",
    "            for fid, cid, t, pid in [l.split(',') for l in f.read().strip().split('\\n')]:\n",
    "                if fid not in fid_dict:\n",
    "                    continue\n",
    "                if cid not in cid_dict:\n",
    "                    cid_dict[cid] = len(cid_dict)\n",
    "                if pid not in pid_dict:\n",
    "                    pid_dict[pid] = len(pid_dict)\n",
    "    pkl.dump(cid_dict, open(cid_dict_path, 'wb'))\n",
    "    pkl.dump(pid_dict, open(pid_dict_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique FileID       : 81894\n",
      "Unique CustomerID   : 5539312\n",
      "Unique ProductID    : 28\n"
     ]
    }
   ],
   "source": [
    "print('%-20s: %d' % ('Unique FileID', len(fid_dict)))\n",
    "print('%-20s: %d' % ('Unique CustomerID', len(cid_dict)))\n",
    "print('%-20s: %d' % ('Unique ProductID', len(pid_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting compressed `query_log`\n",
    "- format for each row: `date`, `fid`, `cid`, `QueryTS`, `pid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new query_log.npy\n",
      "Processing 0531.csv\r"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(npy_path):\n",
    "    print('Loading existed query_log.npy')\n",
    "    query_log = np.load('./dataset/query_log.npy')\n",
    "else:\n",
    "    print('Creating new query_log.npy')\n",
    "    total_row = 0\n",
    "    for fname in sorted(os.listdir(query_log_path)):\n",
    "        print('Processing', fname, end='\\r', flush=True)\n",
    "        if not fname.endswith('.csv'):\n",
    "            continue\n",
    "        with open(os.path.join(query_log_path, fname)) as f:\n",
    "            total_row += len(f.read().strip().split('\\n'))\n",
    "    \n",
    "    query_log = np.zeros((total_row, 5), dtype=np.int32)\n",
    "    top = 0\n",
    "    for fname in sorted(os.listdir(query_log_path)):\n",
    "        print('Processing', fname, end='\\r', flush=True)\n",
    "        if not fname.endswith('.csv'):\n",
    "            continue\n",
    "        date = 20170000 + int(fname[:4])\n",
    "        with open(os.path.join(query_log_path, fname)) as f:\n",
    "            for fid, cid, t, pid in [l.split(',') for l in f.read().strip().split('\\n')]:\n",
    "                if fid not in fid_dict:\n",
    "                    continue\n",
    "                query_log[top, 0] = date\n",
    "                query_log[top, 1] = fid_dict[fid]\n",
    "                query_log[top, 2] = cid_dict[cid]\n",
    "                query_log[top, 3] = int(t)\n",
    "                query_log[top, 4] = pid_dict[pid]\n",
    "                top += 1\n",
    "    np.save('./dataset/query_log', query_log)\n",
    "    \n",
    "\n",
    "print('Total row:', query_log.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
