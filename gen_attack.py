import os
import time
import glob
import pickle as pkl

import scipy
import numpy as np
from sklearn.linear_model import LinearRegression

import features_loader


def get_sort_score(scores):
    to_sort = [(*s, i) for i, s in enumerate(zip(*scores))]
    to_sort.sort()
    
    same_entry = len(scores[0]) - len(set([(*s, ) for s in zip(*scores)]))
    print('Same entry before :', same_entry)
    
    idx = [v[-1] for v in to_sort]
    sort_score = np.zeros(len(to_sort), dtype=np.float64)
    sort_score[idx] = np.arange(len(to_sort)) / (len(to_sort)-1)
    return sort_score


# Loading features & datas
fid_dict = pkl.load(open('./dataset/fid_dict.pkl', 'rb'))

with open('./dataset/training-set.csv') as f:
    train_row = [l.split(',') for l in f.read().strip().split('\n')]
    train_row = [[fid_dict[l[0]], int(l[1])] for l in train_row if l[0] in fid_dict]
    train_dict = dict(train_row)
    train_row = np.array(train_row)
    positive_fid = set(train_row[train_row[:, 1] == 1, 0])
    negative_fid = set(train_row[train_row[:, 1] == 0, 0])

with open('./dataset/testing-set.csv') as f:
    test_row = [l.split(',') for l in f.read().strip().split('\n')]
    test_row = [l for l in test_row if l[0] in fid_dict]

with open('./dataset/features/feature_label_propagate_score.txt') as f:
    lp_score = np.array([l.split() for l in f.read().strip().split('\n')], dtype=np.float64)
    lp_score = lp_score[[fid_dict[FileID] for FileID, _ in test_row]]

X_norm = features_loader.load_X_norm()
X_norm_train = X_norm[:len(train_row)]
y_train = train_row[train_row[:, 0], 1]

linear_reg = LinearRegression(normalize=True)
linear_reg.fit(X_norm_train, y_train)
y_linear_reg_test_ = linear_reg.predict(X_norm[len(train_row):])


# Loading ensembled answers
weight_record = {
    'ensemble_X_embedding_cid': 0.01,
    'ensemble_nn_embedding': 0.01,
    'ensemble_large': 0.01,
    'ensemble_norm': 0.05,
    'ensemble': 1,
    'ensemble_svd256': 1,
    'ensemble_stacking_old': 1,
    'ensemble_stacking': 1,
    'ensemble_svd256_stack_fs': 100,
}

ensembler = []
weight = []
for path in glob.glob('./attack/ensemble_X_embedding_cid/*.txt'):
    with open(path) as f:
        ans = np.array([l.split(',')[1] for l in f.read().strip().split('\n')], dtype=np.float64)
    ensembler.append(ans)
    weight.append(weight_record['ensemble_X_embedding_cid'])

for path in glob.glob('./attack/ensemble_nn_embedding/*.txt'):
    with open(path) as f:
        ans = np.array([l.split(',')[1] for l in f.read().strip().split('\n')], dtype=np.float64)
    ensembler.append(ans)
    weight.append(weight_record['ensemble_nn_embedding'])

for path in glob.glob('./attack/ensemble_large/*.txt'):
    with open(path) as f:
        ans = np.array([l.split(',')[1] for l in f.read().strip().split('\n')], dtype=np.float64)
    ensembler.append(ans)
    weight.append(weight_record['ensemble_large'])

for path in glob.glob('./attack/ensemble_norm/*.txt'):
    ensembler.append(pkl.load(open(path, 'rb')).astype(np.float64))
    weight.append(weight_record['ensemble_norm'])

for path in glob.glob('./attack/ensemble/*.txt'):
    with open(path) as f:
        ans = np.array([l.split(',')[1] for l in f.read().strip().split('\n')], dtype=np.float64)
    ensembler.append(ans)
    weight.append(weight_record['ensemble'])

for path in glob.glob('./attack/ensemble_svd256/*.pkl'):
    ensembler.append(pkl.load(open(path, 'rb')).astype(np.float64))
    weight.append(weight_record['ensemble_svd256'])
    
for path in glob.glob('./attack/ensemble_stacking_old/*.pkl'):
    ensembler.append(pkl.load(open(path, 'rb')).astype(np.float64))
    weight.append(weight_record['ensemble_stacking_old'])
    
for path in glob.glob('./attack/ensemble_stacking/*.pkl'):
    ensembler.append(pkl.load(open(path, 'rb')).astype(np.float64))
    weight.append(weight_record['ensemble_stacking'])

print(np.sum(weight))
for path in glob.glob('./attack/ensemble_svd256_stack_fs/*.pkl'):
    ensembler.append(pkl.load(open(path, 'rb')).astype(np.float64))
    weight.append(weight_record['ensemble_svd256_stack_fs'])
print(np.sum(weight))
    
ensembler = np.array(ensembler, dtype=np.float64)
weight = np.array(weight, dtype=np.float64).reshape(-1, 1)
y_test_ = (ensembler * weight).sum(axis=0) / weight.sum()
print(' y_test_ '.center(60, '='))
print('        mean: %.12f' % (y_test_.mean()))
print('unique entry: %d out of %d' % (len(np.unique(y_test_)), len(test_row)))

# Basic score
print('Output sort score to: /home/sunset/Malware-Detection/daily_ensemble_attack_1.csv')
sort_score = get_sort_score([y_test_, y_linear_reg_test_])
with open('./daily_ensemble_attack_1.csv', 'w') as f:
    for i in range(len(test_row)):
        f.write('%s,%.12f\n' % (test_row[i][0], sort_score[i]))
print('')

# Semi-supervised score
y_test_w_lp_ = np.zeros(len(test_row), dtype=np.float64)
for i in range(len(test_row)):
    lp, base_n, _, g_n, depth, in_edge = lp_score[i]
    y_ = y_test_[i]
    if base_n > 100:
        y_test_w_lp_[i] = (lp + y_ ) / 2
    elif base_n > 1:
        y_test_w_lp_[i] = (lp * base_n + y_) / (base_n + 1)
    else:
        y_test_w_lp_[i] = (0 + y_)

print(' y_test_w_lp_ '.center(60, '='))
print('        mean: %.12f' % (y_test_w_lp_.mean()))
print('unique entry: %d out of %d' % (len(np.unique(y_test_w_lp_)), len(test_row)))

print('Output sort score to: /home/sunset/Malware-Detection/daily_ensemble_attack_2.csv')
sort_score = get_sort_score([y_test_w_lp_, y_linear_reg_test_])
with open('./daily_ensemble_attack_2.csv', 'w') as f:
    for i in range(len(test_row)):
        f.write('%s,%.12f\n' % (test_row[i][0], sort_score[i]))

        
# Generate description
with open('daily_ensemble_attack_description.txt', 'w') as f:
    f.write('daily_ensemble_attack_1.csv\n')
    for k, w in weight_record.items():
        f.write('%s * %.2f\n' % (k, w))
    f.write('\n\n\n')
    
    
    f.write('daily_ensemble_attack_2.csv\n')
    for k, w in weight_record.items():
        f.write('%s * %.2f\n' % (k, w))
    f.write('with lp score\n')
